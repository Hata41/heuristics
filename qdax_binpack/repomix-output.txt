This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-06-06T12:10:40.439Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
behaviours.py
jumanji_conversion.py
nets.py
qdax_jumanji_utils.py
run_qdax_multi_device.py
run_qdax.py
test.py

================================================================
Files
================================================================

================
File: behaviours.py
================
import jumanji
from jumanji.environments.packing.bin_pack.space import Space 
from jumanji.environments.packing.bin_pack.types import Observation, Item, item_volume # Use your actual types

import chex
import jax
import jax.numpy as jnp

from typing import Sequence, Tuple, Union, Optional
from qdax.custom_types import Descriptor
from qdax.core.neuroevolution.buffers.buffer import QDTransition


def _calculate_normalized_rank(chosen_value: chex.Numeric,
                               all_values: chex.Array,
                               valid_mask: chex.Array) -> chex.Numeric:
    """
    Calculates the normalized rank of a chosen value within a set of valid values.
    Rank is [0, 1], where 0 means smallest and 1 means largest.
    Handles cases with few valid values.
    """
    # Consider only valid values for ranking
    values_for_ranking = jnp.where(valid_mask, all_values, -jnp.inf) # Push non-valid to bottom

    num_valid = jnp.sum(valid_mask)

    # Count how many valid values are strictly smaller than the chosen one
    count_smaller = jnp.sum((values_for_ranking < chosen_value) & valid_mask)

    # Normalized rank: (number of items smaller) / (total valid items - 1)
    # This gives 0 for the smallest, 1 for the largest among distinct values.
    # Add epsilon to denominator to avoid division by zero if num_valid is 0 or 1.
    # Score is 0.5 if num_valid <= 1 (undefined or single point)
    score = jax.lax.cond(
        num_valid > 1,
        lambda: count_smaller / (num_valid - 1.0),
        lambda: 0.5, # Neutral score if rank is ill-defined
    )
    return jnp.clip(score, 0.0, 1.0)


def _get_prioritization_scores_single_step(
    obs: Observation,  # Single observation
    flat_action: chex.Numeric, # Single flat action
    num_item_choices_from_spec: int
) -> Tuple[chex.Numeric, chex.Numeric]:
    """
    Calculates item and EMS prioritization scores for a single timestep.
    """
    # Unflatten action: (ems_idx, item_idx)
    # These indices refer to the action space, which should map to obs.ems and obs.items
    item_idx_chosen = flat_action % num_item_choices_from_spec
    ems_idx_chosen = flat_action // num_item_choices_from_spec

    # --- Item Prioritization ---
    # obs.items is an Item Pytree, leaves are (max_num_items,)
    # item_volume will return array of shape (max_num_items,)
    all_item_volumes = item_volume(obs.items)
    chosen_item_volume = all_item_volumes[item_idx_chosen]
    # Mask for items that are generally available (not yet packed, fit instance constraints)
    valid_item_selection_mask = obs.items_mask

    item_prioritization_score = _calculate_normalized_rank(
        chosen_item_volume,
        all_item_volumes,
        valid_item_selection_mask
    )

    # --- EMS Prioritization ---
    # obs.ems is a Space Pytree, leaves are (obs_num_ems,)
    # obs.ems.volume() will return array of shape (obs_num_ems,)
    all_ems_volumes = obs.ems.volume()
    chosen_ems_volume = all_ems_volumes[ems_idx_chosen]
    # Mask for EMSs that are valid (not empty, etc.)
    valid_ems_selection_mask = obs.ems_mask

    ems_prioritization_score = _calculate_normalized_rank(
        chosen_ems_volume,
        all_ems_volumes,
        valid_ems_selection_mask
    )

    return item_prioritization_score, ems_prioritization_score

def binpack_descriptor_extraction(
    data: QDTransition,
    mask: jnp.ndarray,
    num_item_choices_from_spec: int # Changed from env_action_spec
) -> Descriptor:
    """
    Computes descriptors:
    1. Average prioritization of large items.
    2. Average prioritization of large EMSs.
    """
    # num_item_choices_from_spec is now directly an int

    # Vmap over the episode length dimension
    # Input signatures for vmap: (obs_timestep, action_timestep, num_item_choices)
    # obs_timestep is a Pytree, action_timestep is an array.
    # num_item_choices is static, so in_axes is None for it.
    vmapped_scores_over_time = jax.vmap(
        _get_prioritization_scores_single_step,
        in_axes=(0, 0, None), # obs, action are batched along time; num_item_choices is static
        out_axes=0
    )

    # Vmap over the batch dimension (population)
    vmapped_scores_over_batch_time = jax.vmap(
        vmapped_scores_over_time,
        in_axes=(0, 0, None), # obs Pytree, action array are batched; num_item_choices static
        out_axes=0
    )

    # data.obs is a Pytree with leaves of shape (batch_size, episode_length, ...)
    # data.actions is an array of shape (batch_size, episode_length)
    # The third argument to vmapped_scores_over_batch_time is num_item_choices_from_spec
    item_scores_all, ems_scores_all = vmapped_scores_over_batch_time(
        data.obs, data.actions, num_item_choices_from_spec # Pass the int directly
    )
    # ... rest of the function remains the same
    sum_mask = jnp.sum(mask, axis=-1)
    safe_sum_mask = jnp.where(sum_mask == 0, 1.0, sum_mask)

    mean_item_prioritization = jnp.sum(item_scores_all * mask, axis=-1) / safe_sum_mask
    mean_ems_prioritization = jnp.sum(ems_scores_all * mask, axis=-1) / safe_sum_mask
    
    mean_item_prioritization = jnp.where(sum_mask == 0, 0.5, mean_item_prioritization)
    mean_ems_prioritization = jnp.where(sum_mask == 0, 0.5, mean_ems_prioritization)

    descriptors = jnp.stack([mean_item_prioritization, mean_ems_prioritization], axis=-1)
    return descriptors

================
File: jumanji_conversion.py
================
import chex
from typing import NamedTuple
import jax.numpy as jnp
from jumanji.environments.packing.bin_pack.types import Observation, EMS, Item
from jumanji.environments.packing.bin_pack.space import Space

# Define the new NamedTuple for the function's output
class ArrayObservation(NamedTuple):
    """
    An observation structure where all fields are JAX arrays.
    Complex objects like EMS/Space and Item from the original Observation
    are converted into single JAX arrays by stacking their numerical components.
    The field names are identical to the original Observation NamedTuple.
    """
    ems: chex.Array         # e.g., shape (obs_num_ems, 6) or (6,)
    ems_mask: chex.Array    # e.g., shape (obs_num_ems,)
    items: chex.Array       # e.g., shape (max_num_items, 3) or (3,)
    items_mask: chex.Array  # e.g., shape (max_num_items,)
    items_placed: chex.Array# e.g., shape (max_num_items,)
    action_mask: chex.Array # e.g., shape (obs_num_ems, max_num_items)


def observation_to_arrays(obs: Observation) -> ArrayObservation:
    """
    Converts an Observation object (containing potentially nested structures
    like EMS/Space and Item) into an ArrayObservation object where all fields,
    including the representations of EMS and Item, are single JAX arrays.

    - `obs.ems` (an EMS/Space object) is transformed into a JAX array by
      stacking its 6 numerical components (x1, x2, y1, y2, z1, z2) in that order.
    - `obs.items` (an Item object) is transformed into a JAX array by
      stacking its 3 numerical components (x_len, y_len, z_len) in that order.
    - All other fields (originally masks) are ensured to be JAX arrays.

    Args:
        obs: The input Observation object.

    Returns:
        An ArrayObservation object where all fields are JAX arrays.
    """

    # Process obs.ems (which is an EMS, an alias for the Space dataclass)
    # Components are x1, x2, y1, y2, z1, z2.
    # We explicitly list them to ensure a consistent order in the stacked array.
    ems_components_list = [
        jnp.asarray(obs.ems.x1),
        jnp.asarray(obs.ems.x2),
        jnp.asarray(obs.ems.y1),
        jnp.asarray(obs.ems.y2),
        jnp.asarray(obs.ems.z1),
        jnp.asarray(obs.ems.z2),
    ]
    # Stack along the last axis.
    # If input components are 1D (shape (N,)), output is (N, 6).
    # If input components are scalars (shape ()), output is (6,).
    ems_as_array = jnp.stack(ems_components_list, axis=-1)

    # Process obs.items (which is an Item NamedTuple)
    # Components are x_len, y_len, z_len.
    items_components_list = [
        jnp.asarray(obs.items.x_len),
        jnp.asarray(obs.items.y_len),
        jnp.asarray(obs.items.z_len),
    ]
    # Stack along the last axis, similar to EMS.
    # If input components are 1D (shape (M,)), output is (M, 3).
    # If input components are scalars (shape ()), output is (3,).
    items_as_array = jnp.stack(items_components_list, axis=-1)

    # Ensure mask fields are JAX arrays
    # If they are already JAX arrays, jnp.asarray is often a no-op.
    ems_mask_as_array = jnp.asarray(obs.ems_mask)
    items_mask_as_array = jnp.asarray(obs.items_mask)
    items_placed_as_array = jnp.asarray(obs.items_placed)
    action_mask_as_array = jnp.asarray(obs.action_mask)

    # Construct and return the new ArrayObservation object
    return ArrayObservation(
        ems=ems_as_array,
        ems_mask=ems_mask_as_array,
        items=items_as_array,
        items_mask=items_mask_as_array,
        items_placed=items_placed_as_array,
        action_mask=action_mask_as_array,
    )

================
File: nets.py
================
from flax import linen as nn
import jax.numpy as jnp
from typing import Sequence, Tuple, Union, Optional
import jax
import chex
from tensorflow_probability.substrates.jax.distributions import Categorical
from qdax.core.neuroevolution.networks.networks import MLP
from jumanji.environments.packing.bin_pack.types import Observation
from jumanji_conversion import observation_to_arrays


class TransformerBlock(nn.Module):
    """Transformer block with post layer norm, implementing Attention Is All You Need
    [Vaswani et al., 2016].
    EXAMPLE USAGE:
        attention_kwargs = dict(
            num_heads=8,
            qkv_features=16,
            kernel_init=nn.initializers.ones,
            bias_init=nn.initializers.zeros)
    """

    attention_kwargs: dict
    mlp_depth: int = 2

    @nn.compact
    def __call__(
        self,
        query: chex.Array,
        key: chex.Array,
        value: chex.Array,
        mask: Optional[chex.Array] = None,
    ) -> chex.Array:
        """Computes in this order:
            - (optionally masked) MHA with queries, keys & values
            - skip connection
            - layer norm
            - MLP
            - skip connection
            - layer norm

        This module broadcasts over zero or more 'batch-like' leading dimensions.

        Args:
            query: embeddings sequence used to compute queries; shape [..., T', D_q].
            key: embeddings sequence used to compute keys; shape [..., T, D_k].
            value: embeddings sequence used to compute values; shape [..., T, D_v].
            mask: optional mask applied to attention weights; shape [..., H=1, T', T].

        Returns:
            A new sequence of embeddings, consisting of a projection of the
                attention-weighted value projections; shape [..., T', D'].
        """

        # Multi-head attention and residual connection
        attn_output = nn.MultiHeadAttention(**self.attention_kwargs)(
            inputs_q=query, inputs_k=key, inputs_v=value, mask=mask
        )
        h = attn_output + query  # First residual connection
        h = nn.LayerNorm(use_scale=False)(h)

        # MLP and residual connection
        mlp_output = MLP(
            [self.attention_kwargs['qkv_features'] * 2] * self.mlp_depth
            + [self.attention_kwargs['qkv_features']]
        )(h)
        h = mlp_output + h  # Second residual connection
        out = nn.LayerNorm(use_scale=False)(h)
        return out
 
class BPSquashInput(nn.Module):
    """Flattens a Observation."""

    @nn.compact
    def __call__(self, observation) -> chex.Array:
        x = jnp.concatenate([
            observation.ems,
            observation.items,
        ], axis=-1)
        return x

class IdentityInput(nn.Module):
    """Only Observation Input."""

    @nn.compact
    def __call__(self, observation: Observation) -> Observation:
        return observation

class BinPackActor(nn.Module):
    torso: nn.Module
    action_head: nn.Module
    input_layer: nn.Module = IdentityInput()

    @nn.compact
    def __call__(self, observation: Observation) -> jnp.ndarray:
        observation = self.input_layer(observation)
        ems_embeddings, items_embeddings = self.torso(observation)
        
        action_probabilities = self.action_head(
            ems_embeddings, items_embeddings, action_mask=observation.action_mask
        )        
        return action_probabilities

# In nets.py

class BPActorHead(nn.Module):
    # action_dim is not strictly needed here if we infer from logits shape,
    # but can be kept for clarity or future use.
    # action_dim: Union[int, Sequence[int]] = None 
    
    @nn.compact
    def __call__(self, ems_embeddings, items_embeddings, action_mask) -> chex.Array: # Return type is Array, not Categorical
        # ems_embeddings: (..., E, D)
        # items_embeddings: (..., I, D)
        # action_mask: (..., E, I)
        
        logits = jnp.einsum("...ek,...ik->...ei", ems_embeddings, items_embeddings)
        # logits shape: (..., E, I)
        
        masked_logits = jnp.where(action_mask, logits, jnp.finfo(jnp.float32).min)
        
        # Flatten the last two dimensions (E, I) to treat as a single categorical choice
        original_shape = masked_logits.shape
        num_actions_flat = original_shape[-2] * original_shape[-1] # E * I
        
        # Reshape to (..., E * I)
        flattened_logits = masked_logits.reshape(*original_shape[:-2], num_actions_flat)
        
        # Apply softmax over the flattened E*I choices
        action_probabilities_flat = jax.nn.softmax(flattened_logits, axis=-1)
        
        return action_probabilities_flat # Shape: (..., E * I)


class BinPackTorso(nn.Module):
    """attention_kwargs = dict(
        num_heads=transformer_num_heads,
        qkv_features=qkv_features,
        kernel_init=nn.initializers.ones,
        bias_init=nn.initializers.zeros
        )"""
    num_transformer_layers: int
    attention_kwargs : dict
    
    @nn.compact
    def __call__(self, observation) -> Tuple[chex.Array, chex.Array]:

        # Item/EMS encoder
        ems_embeddings = nn.Dense(self.attention_kwargs['qkv_features'])(observation.ems)
        items_embeddings = nn.Dense(self.attention_kwargs['qkv_features'])(observation.items)
        
        # Item/Ems Masks
        items_mask = self._make_self_attention_mask(
            observation.items_mask & ~observation.items_placed
        )
        ems_mask = self._make_self_attention_mask(observation.ems_mask)

        # Decoder
        ems_cross_items_mask = jnp.expand_dims(observation.action_mask, axis=-3)
        items_cross_ems_mask = jnp.expand_dims(
            jnp.moveaxis(observation.action_mask, -1, -2), axis=-3
        )

        for _ in range(self.num_transformer_layers):
            # Self-attention on EMSs.
            ems_embeddings = TransformerBlock(self.attention_kwargs)(ems_embeddings, ems_embeddings, ems_embeddings, ems_mask)
            # Self-attention on items.
            items_embeddings = TransformerBlock(self.attention_kwargs)(items_embeddings, items_embeddings, items_embeddings, items_mask)
            # Cross-attention EMSs on items.
            new_ems_embeddings = TransformerBlock(self.attention_kwargs)(ems_embeddings, items_embeddings, items_embeddings, ems_cross_items_mask)
            # Cross-attention items on EMSs.
            items_embeddings = TransformerBlock(self.attention_kwargs)(items_embeddings, ems_embeddings, ems_embeddings, items_cross_ems_mask)
            ems_embeddings = new_ems_embeddings

        return ems_embeddings, items_embeddings

    def _make_self_attention_mask(self, mask: chex.Array) -> chex.Array:
        # Use the same mask for the query and the key.
        mask = jnp.einsum("...i,...j->...ij", mask, mask)
        # Expand on the head dimension.
        mask = jnp.expand_dims(mask, axis=-3)
        return mask

class Obs_to_Arrays(nn.Module):
    """Only Observation Input."""

    @nn.compact
    def __call__(self, observation: Observation) -> Observation:
        return observation_to_arrays(observation)

================
File: qdax_jumanji_utils.py
================
# qdax_jumanji_utils.py (or whatever you name the file)

from functools import partial
from typing import Any, Callable, Tuple

import flax.linen as nn 
import jax
import jax.numpy as jnp
import jumanji
from chex import ArrayTree 
from typing_extensions import TypeAlias

from qdax.core.neuroevolution.buffers.buffer import QDTransition, Transition
from qdax.custom_types import (
    Descriptor,
    ExtraScores,
    Fitness,
    Genotype,
    Params,
    RNGKey,
)

JumanjiState: TypeAlias = ArrayTree
JumanjiTimeStep: TypeAlias = jumanji.types.TimeStep


def generate_jumanji_unroll(
    init_state: JumanjiState,
    init_timestep: JumanjiTimeStep,
    policy_params: Params,
    key: RNGKey,
    episode_length: int,
    play_step_fn: Callable[
        [JumanjiState, JumanjiTimeStep, Params, RNGKey],
        Tuple[
            JumanjiState,
            JumanjiTimeStep,
            Params,
            RNGKey,
            Transition,
        ],
    ],
) -> Tuple[JumanjiState, JumanjiTimeStep, Transition]:
    """
    Generates an episode (a sequence of transitions) for a single policy
    in a single environment instance.
    """

    def _scan_play_step_fn(
        carry: Tuple[JumanjiState, JumanjiTimeStep, Params, RNGKey], _unused_arg: Any
    ) -> Tuple[Tuple[JumanjiState, JumanjiTimeStep, Params, RNGKey], Transition]:
        env_state_scan, timestep_scan, current_policy_params_scan, current_key_scan = carry
        (
            next_env_state_scan,
            next_timestep_scan,
            _returned_policy_params,
            returned_key_scan,
            transition_output,
        ) = play_step_fn(
            env_state_scan, timestep_scan, current_policy_params_scan, current_key_scan
        )
        next_carry = (
            next_env_state_scan,
            next_timestep_scan,
            current_policy_params_scan,
            returned_key_scan,
        )
        return next_carry, transition_output

    initial_scan_carry = (init_state, init_timestep, policy_params, key)
    final_carry, transitions = jax.lax.scan(
        _scan_play_step_fn,
        initial_scan_carry,
        xs=None,
        length=episode_length,
    )
    final_env_state, final_timestep, _, _ = final_carry
    return final_env_state, final_timestep, transitions


@partial(
    jax.jit,
    static_argnames=(
        "env",
        "n_eval_envs",
        "episode_length",
        "play_step_fn",
        "descriptor_extractor",
    ),
)
def jumanji_scoring_function_eval_multiple_envs(
    policies_params: Genotype,
    eval_batch_key: RNGKey,
    env: jumanji.env.Environment,
    n_eval_envs: int,
    episode_length: int,
    play_step_fn: Callable[
        [JumanjiState, JumanjiTimeStep, Params, RNGKey],
        Tuple[JumanjiState, JumanjiTimeStep, Params, RNGKey, QDTransition],
    ],
    descriptor_extractor: Callable[[QDTransition, jnp.ndarray], Descriptor],
) -> Tuple[Fitness, Descriptor, ExtraScores]:
    """
    Evaluates a batch of policies in parallel using Jumanji environments.
    Each policy is evaluated on `n_eval_envs` newly generated environment
    instances, and the results (fitness, descriptors) are averaged per policy.
    The input `eval_batch_key` is consumed for all stochastic operations within
    this function for the current batch of evaluations.

    Args:
        policies_params: A PyTree of policy parameters. Leaves have a leading
            dimension corresponding to the number of policies.
        eval_batch_key: JAX random key for the entire evaluation batch.
        env: The Jumanji environment instance (used for env.reset).
        n_eval_envs: Number of different environment instances per policy.
        episode_length: Maximum number of steps per episode.
        play_step_fn: Function to play one step in the environment.
        descriptor_extractor: Function to extract behavior descriptors.

    Returns:
        A tuple (fitnesses, descriptors, extra_scores):
            - fitnesses: Averaged fitness per policy (shape: [num_policies]).
            - descriptors: Averaged descriptors per policy 
                           (shape: [num_policies, num_descriptors]).
            - extra_scores: Dictionary, typically including all transitions.
    """
    num_policies = jax.tree.leaves(policies_params)[0].shape[0]
    total_rollouts = num_policies * n_eval_envs

    # Split the eval_batch_key for resets and rollouts within this function
    key_for_resets, key_for_rollouts_pool = jax.random.split(eval_batch_key)
    
    reset_keys_flat = jax.random.split(key_for_resets, total_rollouts)
    rollout_keys_flat = jax.random.split(key_for_rollouts_pool, total_rollouts)

    # Tile/Repeat policy parameters: (P, ...) -> (P*N, ...)
    tiled_policies_params = jax.tree.map(
        lambda x: jnp.repeat(x, n_eval_envs, axis=0), policies_params
    )

    # Reset P*N environments
    vmapped_reset_fn = jax.vmap(env.reset)
    batch_init_states, batch_init_timesteps = vmapped_reset_fn(reset_keys_flat)

    # Partially apply static arguments to the unroll function for vmapping
    unroll_fn_for_vmap = partial(
        generate_jumanji_unroll,
        episode_length=episode_length,
        play_step_fn=play_step_fn,
    )
    
    # Perform all P*N rollouts in parallel
    _final_states, _final_timesteps, all_transitions = jax.vmap(unroll_fn_for_vmap)(
        batch_init_states,
        batch_init_timesteps,
        tiled_policies_params,
        rollout_keys_flat,
    )
    # all_transitions leaves have shape (P*N, episode_length, ...)

    # Create mask for valid steps and calculate fitnesses/descriptors per rollout
    # Ensure dones are float for calculations
    dones_float = all_transitions.dones.astype(jnp.float32)
    is_done_all = jnp.clip(jnp.cumsum(dones_float, axis=1), 0, 1) # (P*N, episode_length)
    
    # Mask for steps that are part of the episode (not after done)
    mask_all = jnp.roll(is_done_all, 1, axis=1) 
    mask_all = mask_all.at[:, 0].set(0.0) # First step is never masked by a previous 'done'

    # Ensure rewards are float
    rewards_float = all_transitions.rewards.astype(jnp.float32) # (P*N, episode_length)
    # Sum rewards over episode length for each of P*N rollouts
    fitnesses_all_rollouts = jnp.sum(rewards_float * (1.0 - mask_all), axis=1) # (P*N,)
    
    # Extract descriptors for each of P*N rollouts
    descriptors_all_rollouts = descriptor_extractor(all_transitions, mask_all) # (P*N, num_descriptors)

    # Reshape and average results per policy
    fitnesses_per_policy_eval = fitnesses_all_rollouts.reshape(
        (num_policies, n_eval_envs)
    ) # (P, N)
    
    num_descriptors = descriptors_all_rollouts.shape[-1]
    descriptors_per_policy_eval = descriptors_all_rollouts.reshape(
        (num_policies, n_eval_envs, num_descriptors)
    ) # (P, N, num_descriptors)

    # Average over the n_eval_envs dimension
    final_fitnesses = jnp.mean(fitnesses_per_policy_eval, axis=1)  # (P,)
    final_descriptors = jnp.mean(
        descriptors_per_policy_eval, axis=1
    )  # (P, num_descriptors)

    # Store all transitions if needed, or other aggregated metrics
    extra_scores = {"transitions_all_rollouts": all_transitions}

    return final_fitnesses, final_descriptors, extra_scores

================
File: run_qdax_multi_device.py
================
import jumanji
from typing import Tuple, Type, Dict, Any, List # Added List

import jax
import jax.numpy as jnp
import functools
import os
import time

from qdax.baselines.genetic_algorithm import GeneticAlgorithm
from qdax.core.distributed_map_elites import DistributedMAPElites
from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids
from qdax.core.neuroevolution.buffers.buffer import QDTransition
from qdax.core.neuroevolution.networks.networks import MLP
from qdax.core.emitters.mutation_operators import isoline_variation
from qdax.core.emitters.standard_emitters import MixingEmitter
from qdax.custom_types import ExtraScores, Fitness, RNGKey, Descriptor, Genotype
from qdax.utils.metrics import default_qd_metrics, CSVLogger

"""New"""
from flax import linen as nn
from nets import BinPackActor, BinPackTorso, BPActorHead, Obs_to_Arrays
from behaviours import binpack_descriptor_extraction
from tqdm import tqdm

# Import the user's custom scoring function utility
try:
    from qdax_jumanji_utils import jumanji_scoring_function_eval_multiple_envs
except ImportError:
    print("WARNING: qdax_jumanji_utils.py not found. Please ensure it's in your PYTHONPATH.")
    print("Using a placeholder function for jumanji_scoring_function_eval_multiple_envs.")
    def jumanji_scoring_function_eval_multiple_envs(genotypes, key, env, n_eval_envs, episode_length, play_step_fn, descriptor_extractor):
        print("Placeholder jumanji_scoring_function_eval_multiple_envs called")
        # Determine num_genotypes correctly for batched PyTrees
        a_leaf = jax.tree_util.tree_leaves(genotypes)[0]
        num_genotypes = a_leaf.shape[0]

        fake_fitnesses = jnp.zeros(num_genotypes)
        fake_descriptors = jnp.zeros((num_genotypes, 2))
        fake_extra_scores = {}
        return fake_fitnesses, fake_descriptors, fake_extra_scores


# Setup JAX devices
try:
    devices = jax.devices('gpu')
except RuntimeError:
    print("GPU not found, using CPU.")
    devices = jax.devices('cpu')
num_devices = len(devices)
print(f'Detected the following {num_devices} device(s): {devices}')

## Define Hyperparameters
seed = 0
episode_length = 20
batch_size_per_device = 1
total_batch_size = batch_size_per_device * num_devices
num_total_iterations = 2000 # Target total algorithm iterations
log_period = 1 # Iterations per compiled update_fn call
num_update_calls = num_total_iterations // log_period # Number of times pmapped update_fn is called
iso_sigma = 0.005
line_sigma = 0.05
N_EVAL_ENVS = 50

## Instantiate the Jumanji environment & Policy
env = jumanji.make('BinPack-v2')
key = jax.random.key(seed)
key, subkey = jax.random.split(key)
action_spec_val = env.action_spec()
NUM_ITEM_CHOICES = action_spec_val.num_values[1].item()
transformer_num_heads, num_transformer_layers, qkv_features = 1, 1, 2
attention_kwargs = dict(num_heads=transformer_num_heads, qkv_features=qkv_features, kernel_init=nn.initializers.ones, bias_init=nn.initializers.zeros)
policy_network = BinPackActor(torso=BinPackTorso(num_transformer_layers=num_transformer_layers, attention_kwargs=attention_kwargs), input_layer=Obs_to_Arrays(), action_head=BPActorHead())

## play_step_fn
def play_step_fn(env_state, timestep, policy_params, key):
    network_input = timestep.observation
    proba_action_flat = policy_network.apply(policy_params, network_input)
    flat_action_idx = jnp.argmax(proba_action_flat, axis=-1)
    chosen_ems_idx, chosen_item_idx = flat_action_idx // NUM_ITEM_CHOICES, flat_action_idx % NUM_ITEM_CHOICES
    env_action = jnp.array([chosen_ems_idx, chosen_item_idx], dtype=jnp.int32)
    next_env_state, next_timestep = env.step(env_state, env_action)
    transition = QDTransition(
        obs=timestep.observation, next_obs=next_timestep.observation, rewards=next_timestep.reward,
        dones=jnp.where(next_timestep.last(), 1.0, 0.0), actions=flat_action_idx,
        truncations=jnp.where(next_timestep.last() & (next_timestep.discount > 0), 1.0, 0.0),
        state_desc=None, next_state_desc=None,
    )
    return next_env_state, next_timestep, policy_params, key, transition

## Init population
key, subkey = jax.random.split(key)
population_keys = jax.random.split(subkey, num=total_batch_size)
obs_spec = env.observation_spec()
single_fake_obs = obs_spec.generate_value()
fake_batch_for_init = jax.tree_util.tree_map(lambda x: x[None, ...], single_fake_obs)
init_variables_flat = jax.vmap(policy_network.init, in_axes=(0, None))(population_keys, fake_batch_for_init)
init_variables = jax.tree_util.tree_map(lambda x: jnp.reshape(x, (num_devices, batch_size_per_device) + x.shape[1:]), init_variables_flat)

## Descriptor extraction & Scoring
descriptor_extraction_fn = functools.partial(binpack_descriptor_extraction, num_item_choices_from_spec=NUM_ITEM_CHOICES)
scoring_fn_dist = functools.partial(
    jumanji_scoring_function_eval_multiple_envs, env=env, n_eval_envs=N_EVAL_ENVS,
    episode_length=episode_length, play_step_fn=play_step_fn, descriptor_extractor=descriptor_extraction_fn,
)
def wrapped_scoring_fn(genotypes: Genotype, key: RNGKey) -> Tuple[Fitness, Descriptor, ExtraScores]:
    fitnesses, descriptors, extra_scores = scoring_fn_dist(genotypes, key)
    return fitnesses.reshape(-1, 1), descriptors, extra_scores

## Emitter & Algorithm
variation_fn = functools.partial(isoline_variation, iso_sigma=iso_sigma, line_sigma=line_sigma)
mixing_emitter = MixingEmitter(mutation_fn=None, variation_fn=variation_fn, variation_percentage=1.0, batch_size=batch_size_per_device)
qd_offset = 0.0
metrics_function = functools.partial(default_qd_metrics, qd_offset=qd_offset)
algo_instance = DistributedMAPElites(scoring_function=wrapped_scoring_fn, emitter=mixing_emitter, metrics_function=metrics_function)

## Centroids & Distributed Init
key, cvt_key = jax.random.split(key)
centroids = compute_cvt_centroids(num_descriptors=2, num_init_cvt_samples=10000, num_centroids=64, minval=0.0, maxval=1.0, key=cvt_key)
key, init_keys_subkey = jax.random.split(key)
distributed_init_keys = jax.random.split(init_keys_subkey, num=num_devices)
distributed_init_keys = jnp.stack(distributed_init_keys)
repertoire, emitter_state, init_metrics_per_device = algo_instance.get_distributed_init_fn(
    centroids=centroids, devices=devices
)(genotypes=init_variables, key=distributed_init_keys)

## Prepare for Metrics Collection
# Initialize all_metrics with empty lists for keys expected from default_qd_metrics
all_metrics: Dict[str, List[Any]] = {
    "qd_score": [], "max_fitness": [], "coverage": []
    # Add other keys here if your metrics_function returns more that you want to plot
}

# Populate initial metrics (from device 0)
logged_init_metrics = {"time": 0.0, "loop": 0, "iteration": 0}
for metric_key, metric_values_all_devices in init_metrics_per_device.items():
    value_from_first_device = metric_values_all_devices[0] # Get value from first device
    logged_init_metrics[metric_key] = value_from_first_device
    if metric_key in all_metrics:
        all_metrics[metric_key].append(value_from_first_device)
    # else: # If a new metric key appears that wasn't pre-defined in all_metrics
    #     all_metrics[metric_key] = [value_from_first_device]


csv_logger = CSVLogger("distributed_mapelites_binpack_logs.csv", header=list(logged_init_metrics.keys()) + ["num_evaluations"]) # Adjusted header
if "num_evaluations" not in logged_init_metrics: logged_init_metrics["num_evaluations"] = total_batch_size * N_EVAL_ENVS # Evals for init
csv_logger.log(logged_init_metrics)

## Get pmapped update function
update_fn = algo_instance.get_distributed_update_fn(num_iterations=log_period, devices=devices)

## Run the optimization loop
print(f"Starting {num_update_calls} update calls, with {log_period} iterations per call.")
actual_evals_done_total = logged_init_metrics.get("num_evaluations",0) # Start with init evals

for i in tqdm(range(num_update_calls), desc="QD Training Progress"):
    start_time = time.time()
    key, loop_key_subkey = jax.random.split(key)
    distributed_loop_keys = jax.random.split(loop_key_subkey, num=num_devices)
    distributed_loop_keys = jnp.stack(distributed_loop_keys)

    repertoire, emitter_state, metrics_from_update_per_device = update_fn(
        repertoire, emitter_state, distributed_loop_keys
    )
    
    current_metrics_first_device = jax.tree_util.tree_map(lambda x: x[0], metrics_from_update_per_device)
    timelapse = time.time() - start_time
    current_qd_iteration = (i + 1) * log_period # QD iteration number at the end of this loop
    
    actual_evals_this_loop = log_period * total_batch_size * N_EVAL_ENVS
    actual_evals_done_total += actual_evals_this_loop

    logged_metrics_csv = {"time": timelapse, "loop": i + 1, "iteration": current_qd_iteration, "num_evaluations": actual_evals_done_total}
    
    for metric_key, value_array_log_period in current_metrics_first_device.items():
        # value_array_log_period has shape (log_period, ...)
        logged_metrics_csv[metric_key] = value_array_log_period[-1] if value_array_log_period.ndim > 0 and len(value_array_log_period) > 0 else value_array_log_period
        if metric_key in all_metrics:
            all_metrics[metric_key].extend(list(value_array_log_period))
        # else: # Handle new metric keys if necessary
        #     all_metrics[metric_key] = list(value_array_log_period)
            
    csv_logger.log(logged_metrics_csv)

print("Training finished.")

# Convert lists in all_metrics to JAX arrays for plotting
for k in list(all_metrics.keys()): # Iterate over copy of keys if modifying dict
    if isinstance(all_metrics[k], list):
        all_metrics[k] = jnp.array(all_metrics[k])
    if not all_metrics[k].shape: # Ensure scalar metrics are at least 1D for plotting consistency
        all_metrics[k] = jnp.array([all_metrics[k]])


## Plotting
final_repertoire = jax.tree_util.tree_map(lambda x: x[0], repertoire)
from qdax.utils.plotting import plot_map_elites_results
from matplotlib.pyplot import savefig
os.makedirs("qdax_binpack_distributed", exist_ok=True)

# Generate x-axis for plotting: iterations [0, 1, 2, ..., actual_iterations_run]
actual_iterations_run = num_update_calls * log_period
# The number of data points for each metric is 1 (init) + actual_iterations_run
plot_x_axis_iterations = jnp.arange(0, actual_iterations_run + 1)
expected_metric_array_len = 1 + actual_iterations_run

# Alternative X-axis: Number of evaluations
# initial_evals = total_batch_size * N_EVAL_ENVS # Evals during init phase
# evals_per_qd_iter_step = total_batch_size * N_EVAL_ENVS # Evals per main algorithm iteration
# plot_x_axis_evals = jnp.concatenate([
#     jnp.array([initial_evals]), # Assuming init evals count for the 0th iteration point
#     initial_evals + (jnp.arange(1, actual_iterations_run + 1) * evals_per_qd_iter_step)
# ])
# Use iteration axis for now, as it's simpler to align with metric collection.
# If you want evaluations, ensure all_metrics contains a reliable 'num_evaluations' series.

metrics_for_plot_filtered = {}
for k, v_array in all_metrics.items():
    if hasattr(v_array, '__len__') and len(v_array) == expected_metric_array_len:
        metrics_for_plot_filtered[k] = v_array
    elif not hasattr(v_array, '__len__') and expected_metric_array_len == 1:
        metrics_for_plot_filtered[k] = jnp.array([v_array])
    else:
        print(f"Plotting Warning: Metric '{k}' length {len(v_array) if hasattr(v_array, '__len__') else 'scalar'} != expected {expected_metric_array_len}. Skipping.")

# Fallback for essential plotting metrics if they got filtered out
for essential_key in ["qd_score", "max_fitness", "coverage"]:
    if essential_key not in metrics_for_plot_filtered:
        print(f"Warning: Essential metric '{essential_key}' missing or mismatched for plotting. Plot may be incomplete or use zeros.")
        if expected_metric_array_len > 0 : # Only add zeros if there's an axis to plot against
             metrics_for_plot_filtered[essential_key] = jnp.zeros(expected_metric_array_len)


if expected_metric_array_len > 0 and \
   all(k in metrics_for_plot_filtered for k in ["qd_score", "max_fitness", "coverage"]):
    fig, axes = plot_map_elites_results(
        env_steps=plot_x_axis_iterations, # Using iteration number as x-axis
        metrics=metrics_for_plot_filtered,
        repertoire=final_repertoire,
        min_descriptor=0.0,
        max_descriptor=1.0,
        x_label="QD Algorithm Iterations (0 = init)" # Updated x-label
    )
    plot_filename = "qdax_binpack_distributed/repertoire_plot_distributed.png"
    savefig(plot_filename)
    print(f"Plot saved to {plot_filename}")
else:
    print("Skipping plotting due to missing essential metrics or zero iterations run.")

================
File: run_qdax.py
================
import jumanji
from typing import Tuple, Type

import jax
import jax.numpy as jnp
import functools

from qdax.baselines.genetic_algorithm import GeneticAlgorithm
from qdax.core.map_elites import MAPElites
from qdax.core.distributed_map_elites import DistributedMAPElites
from qdax.core.containers.mapelites_repertoire import compute_euclidean_centroids, compute_cvt_centroids
from qdax.core.neuroevolution.buffers.buffer import QDTransition
from qdax.core.neuroevolution.networks.networks import MLP
from qdax.tasks.jumanji_envs import jumanji_scoring_function
from qdax.core.emitters.mutation_operators import isoline_variation
from qdax.core.emitters.standard_emitters import MixingEmitter
from qdax.custom_types import ExtraScores, Fitness, RNGKey, Descriptor
from qdax.utils.metrics import default_ga_metrics, default_qd_metrics

"""New"""
from flax import linen as nn
from nets import BinPackActor, BinPackTorso, BPActorHead, Obs_to_Arrays
from behaviours import binpack_descriptor_extraction
from tqdm import tqdm

## Define Hyperparameters

seed = 0
episode_length = 20
population_size = 2
batch_size = population_size

num_iterations = 15

iso_sigma = 0.005
line_sigma = 0.05


## Instantiate the Jumanji environment 

# Instantiate a Jumanji environment using the registry

## Instantiate the Jumanji environment 
env = jumanji.make('BinPack-v2')

# Reset your (jit-able) environment
key = jax.random.key(seed)
key, subkey = jax.random.split(key)
state, timestep = jax.jit(env.reset)(subkey)

# Interact with the (jit-able) environment
action_spec_val = env.action_spec() # Get it once
action = action_spec_val.generate_value()
state, timestep = jax.jit(env.step)(state, action)

# Get number of actions and item choices
num_actions = action_spec_val.num_values.prod().item() # Better way for MultiDiscrete

NUM_ITEM_CHOICES = action_spec_val.num_values[1].item() # e.g., 20

transformer_num_heads = 1
num_transformer_layers = 1
qkv_features = 2
policy_hidden_layer_sizes = (qkv_features)
action_dim = num_actions

attention_kwargs = dict(
        num_heads=transformer_num_heads,
        qkv_features=qkv_features,
        kernel_init=nn.initializers.ones,
        bias_init=nn.initializers.zeros
)

policy_network = BinPackActor(
    torso=BinPackTorso(
            num_transformer_layers = num_transformer_layers,
            attention_kwargs = attention_kwargs),
    input_layer=Obs_to_Arrays(),
    action_head=BPActorHead()
    )

## Utils to interact with the environment
#Define a way to process the observation and define a way to play a step in the environment, given the parameters of a policy_network.

def observation_processing(observation):
    return observation

def play_step_fn(
    env_state,
    timestep,
    policy_params,
    key,
):
    network_input = observation_processing(timestep.observation)
    proba_action_flat = policy_network.apply(policy_params, network_input)
    flat_action_idx = jnp.argmax(proba_action_flat, axis=-1)
    
    # Use the globally defined NUM_ITEM_CHOICES
    chosen_ems_idx = flat_action_idx // NUM_ITEM_CHOICES
    chosen_item_idx = flat_action_idx % NUM_ITEM_CHOICES
    
    env_action = jnp.array([chosen_ems_idx, chosen_item_idx], dtype=jnp.int32)
    
    state_desc = None
    next_state, next_timestep = env.step(env_state, env_action)

    next_state_desc = None

    transition = QDTransition(
        obs=timestep.observation,
        next_obs=next_timestep.observation,
        rewards=next_timestep.reward,
        dones=jnp.where(next_timestep.last(), jnp.array(1.0), jnp.array(0.0)),
        actions=flat_action_idx,
        truncations=jnp.array(0.0),
        state_desc=state_desc,
        next_state_desc=next_state_desc,
    )

    return next_state, next_timestep, policy_params, key, transition
## Init a population of policies
#Also init init states and timesteps


# Init population of controllers
key, subkey = jax.random.split(key)
# Create one PRNG key for each policy in the population
population_keys = jax.random.split(subkey, num=population_size) # Changed from batch_size to population_size

# compute observation size from observation spec (not needed for descriptor anymore)
obs_spec = env.observation_spec()

# Generate a fake batch for a SINGLE instance to get shapes for init
# Your policy_network.init expects a batch dimension for the observation.
# So, fake_batch should have a leading dimension of 1.
single_fake_obs = obs_spec.generate_value()
fake_batch_for_init = jax.tree_util.tree_map(lambda x: x[None, ...], single_fake_obs)

# Initialize a population of policy networks using jax.vmap
# The vmap is over the keys. The fake_batch_for_init is broadcasted.
init_variables = jax.vmap(policy_network.init, in_axes=(0, None))(
    population_keys,
    fake_batch_for_init
)
# Now, init_variables will be a PyTree where each leaf parameter
# has a leading dimension of `population_size`.

# Create the initial environment states
key, subkey = jax.random.split(key)
# Keys for resetting the environment, one for each member of the population
env_reset_keys = jax.random.split(subkey, num=population_size) # Match population_size
reset_fn = jax.jit(jax.vmap(env.reset))

init_states, init_timesteps = reset_fn(env_reset_keys)

## Define a method to extract descriptor when relevant

descriptor_extraction_fn = functools.partial(
    binpack_descriptor_extraction,
    num_item_choices_from_spec=NUM_ITEM_CHOICES # Pass the Python int
)

from qdax_jumanji_utils import jumanji_scoring_function_eval_multiple_envs # Import the new function
N_EVAL_ENVS = 1
scoring_fn = functools.partial(
    jumanji_scoring_function_eval_multiple_envs,
    env=env,                             # Pass the env instance (static)
    n_eval_envs=N_EVAL_ENVS,             # Pass num envs per policy (static)
    episode_length=episode_length,       # Pass episode length (static)
    play_step_fn=play_step_fn,           # Pass play_step_fn (static)
    descriptor_extractor=descriptor_extraction_fn, # Pass descriptor_extractor (static)
)

# The scoring_fn definition remains the same, as it takes descriptor_extractor as an argument
# scoring_fn = functools.partial(
#     jumanji_scoring_function,
#     init_states=init_states,
#     init_timesteps=init_timesteps,
#     episode_length=episode_length,
#     play_step_fn=play_step_fn,
#     descriptor_extractor=descriptor_extraction_fn, # This now uses your new function
# )

## Define Scoring Function
def scoring_function(
    genotypes: jnp.ndarray, key: RNGKey
) -> Tuple[Fitness, ExtraScores, RNGKey]:
    fitnesses, _, extra_scores = scoring_fn(genotypes, key)
    return fitnesses.reshape(-1, 1), extra_scores

##  Define emitter
variation_fn = functools.partial(
    isoline_variation, iso_sigma=iso_sigma, line_sigma=line_sigma
)
mixing_emitter = MixingEmitter(
    mutation_fn=None,
    variation_fn=variation_fn,
    variation_percentage=1.0,
    batch_size=batch_size
)

## Define the algorithm used and apply the initial step
#One can either use a simple genetic algorithm or use MAP-Elites.

use_map_elites = True

if not use_map_elites:
    algo_instance = GeneticAlgorithm(
        scoring_function=scoring_function,
        emitter=mixing_emitter,
        metrics_function=default_ga_metrics,
    )

    key, subkey = jax.random.split(key)
    repertoire, emitter_state, init_metrics = algo_instance.init(
        init_variables, population_size, subkey
    )

else:
    # Define a metrics function
    metrics_function = functools.partial(
        default_qd_metrics,
        qd_offset=0,
    )

    ## Instantiate MAP-Elites
    algo_instance = MAPElites(
        scoring_function=scoring_fn,
        emitter=mixing_emitter,
        metrics_function=metrics_function,
    )
    
    # ## Instantiate Distributed MAP-Elites
    # algo_instance = DistributedMAPElites(
    # scoring_function=scoring_function,
    # emitter=mixing_emitter,
    # metrics_function=metrics_function,
    # )
    
    # # Compute the centroids
    # centroids = compute_euclidean_centroids(
    #     grid_shape=(10, 10),
    #     minval=0.0,
    #     maxval=1.0,
    # )
    
    #############
    ## This uses the Vornoi Tessalaiton and not a simple grid
    cvt_key = jax.random.key(seed)

    # Compute the centroids
    centroids = compute_cvt_centroids(
    num_descriptors = 2,
    num_init_cvt_samples= 100,
    num_centroids =64,
    minval= 0.0,
    maxval= 1.0,
    key = cvt_key,
    )
    
    #############

    # Compute initial repertoire and emitter state
    key, subkey = jax.random.split(key)
    repertoire, emitter_state, init_metrics = algo_instance.init(init_variables, centroids, subkey)
    
## Run the optimization loop
# Run the algorithm

(repertoire, emitter_state, key,), metrics = jax.lax.scan(
    algo_instance.scan_update,
    (repertoire, emitter_state, key),
    (),
    length=num_iterations,
)

## Plotting

from qdax.utils.plotting import plot_map_elites_results
from matplotlib.pyplot import savefig

# create the x-axis array
env_steps = jnp.arange(num_iterations) * episode_length * batch_size

# create the plots and the grid
fig, axes = plot_map_elites_results(
    env_steps=env_steps, 
    metrics=metrics, 
    repertoire=repertoire, 
    min_descriptor=0.0, 
    max_descriptor=1.0
)
savefig("qdax_binpack/repertoire_plot.png")

================
File: test.py
================
import jax
import functools
from qdax.core.map_elites import MAPElites
from qdax.core.containers.mapelites_repertoire import compute_euclidean_centroids
from qdax.tasks.arm import arm_scoring_function
from qdax.core.emitters.mutation_operators import isoline_variation
from qdax.core.emitters.standard_emitters import MixingEmitter
from qdax.utils.metrics import default_qd_metrics

seed = 42
num_param_dimensions = 100  # num DoF arm
init_batch_size = 100
batch_size = 1024
num_iterations = 50
grid_shape = (100, 100)
min_param = 0.0
max_param = 1.0
min_descriptor = 0.0
max_descriptor = 1.0

# Init a random key
key = jax.random.key(seed)

# Init population of controllers
key, subkey = jax.random.split(key)
init_variables = jax.random.uniform(
    subkey,
    shape=(init_batch_size, num_param_dimensions),
    minval=min_param,
    maxval=max_param,
)

# Define emitter
variation_fn = functools.partial(
    isoline_variation,
    iso_sigma=0.05,
    line_sigma=0.1,
    minval=min_param,
    maxval=max_param,
)
mixing_emitter = MixingEmitter(
    mutation_fn=lambda x, y: (x, y),
    variation_fn=variation_fn,
    variation_percentage=1.0,
    batch_size=batch_size,
)

# Define a metrics function
metrics_fn = functools.partial(
    default_qd_metrics,
    qd_offset=0.0,
)

# Instantiate MAP-Elites
map_elites = MAPElites(
    scoring_function=arm_scoring_function,
    emitter=mixing_emitter,
    metrics_function=metrics_fn,
)

# Compute the centroids
centroids = compute_euclidean_centroids(
    grid_shape=grid_shape,
    minval=min_descriptor,
    maxval=max_descriptor,
)

# Initializes repertoire and emitter state
key, subkey = jax.random.split(key)
repertoire, emitter_state, metrics = map_elites.init(init_variables, centroids, subkey)

# Jit the update function for faster iterations
update_fn = jax.jit(map_elites.update)

# Run MAP-Elites loop
for i in range(num_iterations):
    key, subkey = jax.random.split(key)
    (repertoire, emitter_state, metrics,) = update_fn(
        repertoire,
        emitter_state,
        subkey,
    )

# Get contents of repertoire
repertoire.genotypes, repertoire.fitnesses, repertoire.descriptors
